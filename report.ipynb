{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Basics of Mobile Robotics\n",
    "\n",
    "Report of Jean, Vadim, Sara and Lisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents:**\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Vision](#Vision)\n",
    "3. [Global Path Planning](#Global-Path-Planning)\n",
    "4. [Localization with Kalman Filter](#Kalman-Filter)\n",
    "5. [Motor Control](#Motor-Control)\n",
    "6. [Local Obstacle Avoidance](#Local-Obstacle-Avoidance)\n",
    "7. [Putting it all together](#Putting-it-all-together)\n",
    "8. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Describe our general project idea. Introduction to the environement and choices we made. General function of the main components.\n",
    "\n",
    "Structure of the code\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Code required to execute vision module independently. Don't describe the code but explain what is behind it (i.e. theory, algorithms, measurements, choice of parameters, etc.).\n",
    "\n",
    "CITE YOUR SOURCES\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's start by defining what main functionalities the vision module should accomplish.\n",
    "1. Find the **borders of the map** in the image.\n",
    "2. Detect the **global obstacles**, which are placed on the map.\n",
    "3. Locate the **position of the goal** in the map.\n",
    "4. Locate the **position of the robot** in the map.\n",
    "\n",
    "As our camera has fixed position points 1,2 and 4 only need to be done once during the initalization of the map, while the localization of the robot will be used during runtime as a measurement for the Kalman filter.\n",
    "\n",
    "Additionally a visualition during runtime of all the above points, as well as the estimated robot position given by the filter and the planned path will ease the development further.\n",
    "\n",
    "All the code used for the vision module can be found under `src/vision`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borders of the map\n",
    "\n",
    "To find the borders of the map, we decided to use [aruco markers](https://www.uco.es/investiga/grupos/ava/portfolio/aruco/l) - a robust library available for open cv to detect predefined markers in an image used for pose estimation - to mark the corners of our map.\n",
    "\n",
    "For this we find the first corner of each four corner markers (markers have ids 1-4). We then apply a perspective transform where we map the four given corners to predefined image of size 350x240 pixel. The choice of the cropped map size is based on the actual size of our image, which is 1050mmx720mm, so every pixel in our cropped map corresponds to 3 mm in reality.\n",
    "\n",
    "The principle can be seen on this image:\n",
    "\n",
    "<img src=\"./media/perspective_transform.png\" alt=\"Perspective Transform\" width=\"500\"/>\n",
    "\n",
    "The Perspective transform allows us to rectify the map in our image. As the camera looks at the scene with a certain angle, the map appears distorted on the image. The perspective transform projects the map again onto a regular rectangle. It though does not correct for any radial distortion of the camera itself. For this the camera would need to be calibrated but since the precision without this step was sufficient for our purposes, we skipped this step.\n",
    "\n",
    "This whole process is done in the [camera class](src/vision/camera.py) in `src/vision/camera.py::_find_corners()`, the for illustration purposes required code is copied below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from src.vision.helpers import get_corner, perspective_transform, Hyperparameters, read_yaml\n",
    "\n",
    "example_img_path = \"media/example_map.jpg\"\n",
    "frame = cv2.imread(example_img_path)\n",
    "global_corners = [None, None, None, None]\n",
    "aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_250)\n",
    "detector = cv2.aruco.ArucoDetector(aruco_dict, cv2.aruco.DetectorParameters())\n",
    "hyperparams = Hyperparameters(read_yaml())\n",
    "\n",
    "# This function is taken from src/vision/camera.py and modified to make it work without\n",
    "# the class context.\n",
    "def _find_corners(frame, aruco_detecter, show=False):\n",
    "    \"\"\"\n",
    "    Detects the aruco markers of the corners and updates the position\n",
    "\n",
    "    Args:\n",
    "        show (bool): If True shows the frame with drawn markers. Default: False\n",
    "    \"\"\"\n",
    "    if show:\n",
    "        img = frame.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    corners, ids, rejected = aruco_detecter.detectMarkers(gray)\n",
    "    if ids is not None:\n",
    "        # the 4 corner of our map have ids 1,2,3,4\n",
    "        for i, id in enumerate(ids):\n",
    "            if id >= 1 and id <= 4:  # only consider corner markers\n",
    "                global_corners[id[0]-1] = get_corner(corners[i])\n",
    "        if show:\n",
    "            cv2.aruco.drawDetectedMarkers(img, corners, ids)\n",
    "    \n",
    "    if show:\n",
    "        return img\n",
    "    \n",
    "    \n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 6), sharey='row')  \n",
    "axes[0, 0].set_title(\"Raw Frame\")\n",
    "axes[0, 0].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 1].set_title(\"Detected Aruco markers in the image\")\n",
    "markers = _find_corners(frame, detector, show=True)\n",
    "axes[0, 1].imshow(cv2.cvtColor(markers, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 0].set_title(\"Transformed image\")\n",
    "warped = perspective_transform(frame, global_corners, hyperparams.map_size[0], hyperparams.map_size[1])\n",
    "axes[1, 0].imshow(cv2.cvtColor(warped, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global obstacles\n",
    "\n",
    "The global (black) obstacles are extracted from the imag of the map by:\n",
    "1. thresholding the image to extract the black pixels\n",
    "2. using a canny edge detector to find the edges\n",
    "3. finding the contours and filtering them by area\n",
    "\n",
    "**Thresholding**:\n",
    "\n",
    "Thresholding is the simplest method of image segmentation where every pixel value above, below or within certain thresholds is set to a certain value. As our obstaces are black, we threshold on the BGR color space and set every pixel with a value below a certain threshold to black. The extracted obstacles appear now as white on a binary image. \n",
    "The thresholding step has three hyperparameters, one for each color channel.\n",
    "\n",
    "__Canny Edge Detection__:\n",
    "\n",
    "The Canny edge detector is an algorithm to detect various types of edges in an image. We used it to detect the edges of the binary image of the obstacles.\n",
    "\n",
    "__Contours__:\n",
    "\n",
    "From the edges we can find the contour of the obstacle, which is a set of boundary points of the obstacle. Since the _findContours_ function in opencv is sensitive to non-closed contours, we use a morphology operation to close the contours before applying the function. This step significantly improved the robustness of the contour detection.\n",
    "To filter for noise we only keep contours with an area above a certain threshold.\n",
    "\n",
    "The function `src/vision/camera.py::_extract_obstacles()` contains the code reuiqred for this step and saves a map with the detected obstacles drawn on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_obstacles(cropped_img, hyperparams: Hyperparameters, show_warped: bool = False):\n",
    "    \"\"\"\n",
    "    Thresholds the current frame to see the black obstacles.\n",
    "\n",
    "    Args:\n",
    "        show_warped (bool): If True shows the cropped image with drawn contours. \n",
    "                            Default: False.\n",
    "    \"\"\"\n",
    "    warped = cropped_img.copy()\n",
    "    thresholded = cv2.inRange(warped, (0, 0, 0), \n",
    "                                (hyperparams.obstacles.blue, \n",
    "                                hyperparams.obstacles.green,\n",
    "                                hyperparams.obstacles.red))\n",
    "    \n",
    "    canny = cv2.Canny(thresholded, 94, 98, apertureSize=3)\n",
    "    kernel = np.ones((hyperparams.obstacles.kernel_size, \n",
    "                      hyperparams.obstacles.kernel_size), np.uint8)\n",
    "    morph = cv2.morphologyEx(canny, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "    contours, _ = cv2.findContours(morph, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # filter controus by area\n",
    "    contours = [cnt for cnt in contours if cv2.contourArea(cnt) > hyperparams.obstacles.area]\n",
    "    map = np.zeros_like(warped, dtype=np.uint8)\n",
    "    \n",
    "    cv2.drawContours(map, contours, -1, (255, 255, 255), cv2.FILLED)\n",
    "    if show_warped:\n",
    "        cv2.drawContours(warped, contours, -1, (0, 0, 255), 1)\n",
    "        return warped, map\n",
    "    return map\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(8, 6), sharey='row')\n",
    "\n",
    "axes[0].set_title(\"Cropped Frame\")\n",
    "axes[0].imshow(cv2.cvtColor(warped, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "axes[1].set_title(\"Detected Contours\")\n",
    "contours, map = _extract_obstacles(warped, hyperparams, show_warped=True)\n",
    "axes[1].imshow(cv2.cvtColor(contours, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "axes[2].set_title(\"Binary Obstacle Map\")\n",
    "axes[2].imshow(cv2.cvtColor(map, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal Position\n",
    "\n",
    "To find the goal in the map we use a similar approach as before with different thresholds for BGR color space. As our goal is red we want the red channel to be above a certain value and the blue and green channel to be below a certain value. The goal is then the center of the contour in the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_goal(cropped_img, hyperparams: Hyperparameters, show_warped: bool = False):\n",
    "        \"\"\"\n",
    "        Thresholds the current frame to extract the goal position.\n",
    "\n",
    "        Args:\n",
    "            show_warped (bool): If True shows the cropped image with drawn contour. \n",
    "                                Default: False.\n",
    "        \"\"\"\n",
    "        warped = cropped_img.copy()\n",
    "        thresholded = cv2.inRange(warped, (0, 0, hyperparams.goal.red), \n",
    "                                    (hyperparams.goal.blue, \n",
    "                                     hyperparams.goal.green,255))\n",
    "        \n",
    "        canny = cv2.Canny(thresholded, 94, 98, apertureSize=3)\n",
    "        kernel = np.ones((hyperparams.goal.kernel_size, \n",
    "                          hyperparams.goal.kernel_size), np.uint8)\n",
    "        morph = cv2.morphologyEx(canny, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "        contours, _ = cv2.findContours(morph, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # filter controus where area < 500\n",
    "        contours = [cnt for cnt in contours if cv2.contourArea(cnt) > hyperparams.goal.area]\n",
    "        if contours:\n",
    "            M = cv2.moments(contours[0])\n",
    "            cx = int(M['m10']/M['m00'])\n",
    "            cy = int(M['m01']/M['m00'])\n",
    "            goal_position = [cx, cy]\n",
    "        \n",
    "        if show_warped:\n",
    "            cv2.drawContours(warped, contours, -1, (0, 0, 0), 2)\n",
    "            cv2.circle(warped, goal_position, 5, (255, 0, 0), cv2.FILLED)\n",
    "            return warped, goal_position\n",
    " \n",
    "contours, goal = _extract_goal(warped, hyperparams, show_warped=True)\n",
    "print(f\"Goal position: {goal}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 6), sharey='row')\n",
    "\n",
    "axes[0].set_title(\"Cropped Frame\")\n",
    "axes[0].imshow(cv2.cvtColor(warped, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "axes[1].set_title(\"Detected Goal Position\")\n",
    "axes[1].imshow(cv2.cvtColor(contours, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Get better example image with goood thresholds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot Position\n",
    "\n",
    "The robot position is found by detection the arcuo marker with id 42 which is taped on top of the robot. The postion of the robot is the center of the marker, whereas the orientation is the angle formed between the marker and the x-axis. \n",
    "\n",
    "The center and orientation can be calculated from the four corners of the markers (given by the aruco library) as follows:\n",
    "\n",
    "<img src=\"./media/robot_pose.png\" alt=\"Robot Pose Calculation\" width=\"600\"/>\n",
    "\n",
    "As the aruco marker is not detected in every frame and the orientation is influenced by noise, we use a moving average filter to smoothen the results. In case the the robot is not detected for several consecutive frames, the function returns `np.nan` for the position and orientation. This serves as a flag for the Kalman filter to ignore the measurement in case the camera is hidden.\n",
    "The specific code can be found [here](src/vision/measurements.py).\n",
    "\n",
    "The function `src/vision/camera.py::_extract_robot_pose()` contains the code required for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vision.measurements import Orientation, Position\n",
    "def _extract_robot_pose(cropped_img, aruco_detector, robot_orientation: Orientation, \n",
    "                        robot_position: Position, show: bool = False):\n",
    "    \"\"\"\n",
    "    Extracts the robot pose from latest frame\n",
    "    Args:\n",
    "        show (bool): If True shows the found aruco marker\n",
    "    \"\"\"\n",
    "    warped = cropped_img.copy() \n",
    "    if show:\n",
    "        img = warped.copy()\n",
    "    gray = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    corners, ids, rejected = aruco_detector.detectMarkers(gray)\n",
    "    if ids is not None:\n",
    "        # the 4 corner of our map have ids 1,2,3,4\n",
    "        for i, id in enumerate(ids):\n",
    "            if id == 42:  # only consider corner markers\n",
    "                c1 = corners[0][0][0].astype(int)\n",
    "                c2 = corners[0][0][1].astype(int)\n",
    "                c3 = corners[0][0][2].astype(int)\n",
    "                center = np.mean([c1, c3], axis=0)\n",
    "                p = np.mean([c1, c2], axis=0)\n",
    "                # angle = np.arctan2((center-p)[1], (p-center)[0])\n",
    "                robot_orientation.update(center-p)\n",
    "                robot_position.update(center)\n",
    "            if show:\n",
    "                cv2.aruco.drawDetectedMarkers(img, corners, ids)\n",
    "    else:\n",
    "        # if no aruco marker found update with nan\n",
    "        robot_position.update([np.nan, np.nan])\n",
    "        robot_orientation.update([np.nan, np.nan])\n",
    "        \n",
    "    if show:\n",
    "        if not np.isnan(robot_orientation.value):\n",
    "            cv2.putText(img, str(np.degrees(robot_orientation.value).astype(int)), (50, 50), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0))\n",
    "        return img\n",
    "    \n",
    "robot_orientation = Orientation()\n",
    "robot_position = Position()\n",
    "robot_pose = _extract_robot_pose(warped, detector, robot_orientation, robot_position, show=True)\n",
    "print(f\"Robot position: {robot_position.value}, Robot orientation: {np.degrees(robot_orientation.value):.2f}°\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 6), sharey='row')\n",
    "\n",
    "axes[0].set_title(\"Cropped Frame\")\n",
    "axes[0].imshow(cv2.cvtColor(warped, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "axes[1].set_title(\"Detected Robot Pose and Orientation\")\n",
    "axes[1].imshow(cv2.cvtColor(robot_pose, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "The class camera provides a function `src/vision/camera.py::visualize_map()` which draws the map with all the detected elements on it, as well as the estimated robot position from the kalman filter and the optimal path if inialized. The final visualization can be seen in the gif below.\n",
    "\n",
    "<img src=\"./media/example_visualization.gif\" alt=\"Vision Visualization\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little word about hyperparameters\n",
    "\n",
    "Vision depends strongly on the lightning conditions, which change from place to place but also during the day. To ease the tuning for those hyperparameters we created a small script `scripts/threshold_finder.py` which allows to find the correct combination of parameters with live feedback from the camera. The thresholds found are then stored in [src/vision/values](src/vision/values.yml) and loaded during runtime of the main program. \n",
    "\n",
    "A screenshot of the script can be seen below:\n",
    "\n",
    "<p align=\"top\">\n",
    "    <img src=\"media/threshold_finder.png\" alt=\"Threshold Finder\" width=\"600\"/> \n",
    "    <img src=\"media/threshold_finder_settings.png\" alt=\"Threshold Finder Settings\" width=\"400\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Path Planning\n",
    "\n",
    "Code required to execute vision module independently. Don't describe the code but explain what is behind it (i.e. theory, algorithms, measurements, choice of parameters, etc.).\n",
    "\n",
    "CITE YOUR SOURCES\n",
    "\n",
    "After the extracting the binary matrix of navigable terrain, as well as the positions of the robot and the target, we inflate the obstacles (explain the algo) by the size equal to the maximum distance between the robot's center of rotation and its side. This ensures that for all possible poses the robot does not collide with obstacles.\n",
    "\n",
    "[code: obstacle inflation]\n",
    "[plot: inflated obstacles] \n",
    "\n",
    "\n",
    "After the matrix is calculated, we use the A-star algorithm to find the shortest path between the the robot and target positions. (optional: intermedaite checkpoint elimination by Sara).\n",
    "\n",
    "We then delete all the redundant points from the checkpoint list using a \"visibility algorithm\" (elaborate further). NOTE: why A-star before visibility and not the other way around? Because we don't assume polygonal obstacles so it's not necessarily more robust nor efficient in computation. In the end we are left with a path optimized in terms of both distance and number of checkpoints: \n",
    "\n",
    "[code: visibility algo]\n",
    "[plot: final checkpoints list]\n",
    "[plot: final checkpoints list over camera image]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman Filter\n",
    "\n",
    "# Extended Kalman Filter for Robot Localization\n",
    "\n",
    "This report provides an explanation of the implementation of the Extended Kalman Filter (EKF) used for robot localization. The EKF estimates the robot's position, orientation, and motion parameters based on sensor inputs (camera data) and motor control signals.\n",
    "\n",
    "## Introduction to Kalman Filtering\n",
    "The Kalman Filter is a recursive estimation algorithm used to infer the state of a dynamic system. It is particularly effective for systems with noisy measurements and process models. The **Extended Kalman Filter (EKF)** is a nonlinear extension of the Kalman Filter, suitable for systems where the dynamics or measurement models are nonlinear.\n",
    "\n",
    "In this implementation, the EKF is used to estimate the robot's state vector:\n",
    "\n",
    "- **Position**: \\((x, y)\\) coordinates in the environment\n",
    "- **Orientation**: Angle \\(\\theta\\) (yaw, in radians)\n",
    "- **Velocity**: Linear velocity \\(v\\)\n",
    "- **Angular velocity**: Rate of change of \\(\\theta\\)\n",
    "\n",
    "CITE YOUR SOURCES\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motor Control\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Code required to execute vision module independently. Don't describe the code but explain what is behind it (i.e. theory, algorithms, measurements, choice of parameters, etc.).\n",
    "\n",
    "CITE YOUR SOURCES\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Obstacle Avoidance\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Code required to execute vision module independently. Don't describe the code but explain what is behind it (i.e. theory, algorithms, measurements, choice of parameters, etc.).\n",
    "\n",
    "CITE YOUR SOURCES\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "<span style=\"color:red\">\n",
    "A section which is used to run the overall project and where we can see the path chosen, where the system believes the robot is along the path before and after filtering etc… \n",
    "\n",
    "REFERENCE OT THE MAIN FILE.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "<span style=\"color:red\">\n",
    "What we learned, what we would do differently, what we would like to add, etc.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "(link documentation of used python functions that are not immediately clear)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bomr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
